{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2730d638-9baf-45aa-9c66-443c58273363",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "edde47ae-01c3-430e-853e-4d1550bb8fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "10c8635d-020c-49a9-ae22-c5da5575e5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image_path, max_size=400, shape=None):\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "\n",
    "    if max(image.size) > max_size:\n",
    "        size = max_size\n",
    "    else:\n",
    "        size = max(image.size)\n",
    "    \n",
    "    if shape:\n",
    "        size = shape\n",
    "\n",
    "    in_transform = transforms.Compose([\n",
    "        transforms.Resize(size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.485, 0.456, 0.406), \n",
    "                             (0.229, 0.224, 0.225))])\n",
    "\n",
    "    image = in_transform(image)[:3, :, :].unsqueeze(0)\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0dcd7b9b-90bd-4331-8e70-aca72c87d9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(tensor, title=None):\n",
    "    image = tensor.clone().cpu().detach().numpy().squeeze()\n",
    "    image = image.transpose(1, 2, 0)\n",
    "    image = image * (0.229, 0.224, 0.225) + (0.485, 0.456, 0.406)\n",
    "    image = image.clip(0, 1)\n",
    "    plt.imshow(image)\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    plt.pause(0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d20da7a7-fa90-4568-9a89-ad3500b34927",
   "metadata": {},
   "outputs": [],
   "source": [
    "content = load_image(\"C:/Users/tripa/Desktop/Neural Style Transfer Model/project/Components/content/YellowLabradorLooking_new.jpg\")\n",
    "style = load_image(\"C:/Users/tripa/Desktop/Neural Style Transfer Model/project/Components/style/Vassily_Kandinsky,_1913_-_Composition_7.jpg\", shape=content.shape[-2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "bd287906-0cfc-4f95-a91c-14c0784ef628",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGG, self).__init__()\n",
    "        self.chosen_features = ['0', '5', '10', '19', '28']\n",
    "        self.model = models.vgg19(pretrained=True).features[:29]\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = []\n",
    "        for layer_num, layer in enumerate(self.model):\n",
    "            x = layer(x)\n",
    "            if str(layer_num) in self.chosen_features:\n",
    "                features.append(x)\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "da794157-45bd-4853-b621-d3ab4b95276f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def content_loss(target_features, content_features):\n",
    "    return torch.mean((target_features - content_features) ** 2)\n",
    "\n",
    "def gram_matrix(tensor):\n",
    "    _, d, h, w = tensor.size()\n",
    "    tensor = tensor.view(d, h * w)\n",
    "    gram = torch.mm(tensor, tensor.t())\n",
    "    return gram \n",
    "\n",
    "def style_loss(target_features, style_grams):\n",
    "    style_loss = 0\n",
    "    for target_feature, style_gram in zip(target_features, style_grams):\n",
    "        _, d, h, w = target_feature.shape\n",
    "        target_gram = gram_matrix(target_feature)\n",
    "        layer_style_loss = torch.mean((target_gram - style_gram) ** 2)\n",
    "        style_loss += layer_style_loss / (d * h * w)\n",
    "    return style_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "1996afc7-8935-4b57-a40a-a731bb8aa349",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_style_transfer(content_img, style_img, content_weight=1e5, style_weight=1e10, num_steps=300):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    vgg = VGG().to(device).eval()\n",
    "    \n",
    "    content_img = content_img.to(device)\n",
    "    style_img = style_img.to(device)\n",
    "    target_img = content_img.clone().requires_grad_(True).to(device)\n",
    "\n",
    "    # Define optimizer\n",
    "    optimizer = optim.LBFGS([target_img.requires_grad_()])\n",
    "\n",
    "    style_features = vgg(style_img)\n",
    "    content_features = vgg(content_img)\n",
    "    style_grams = [gram_matrix(feature) for feature in style_features]\n",
    "\n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        target_features = vgg(target_img)\n",
    "        c_loss = content_loss(target_features[1], content_features[1])\n",
    "        s_loss = style_loss(target_features, style_grams)\n",
    "        total_loss = content_weight * c_loss + style_weight * s_loss\n",
    "        total_loss.backward(retain_graph=True)\n",
    "        return total_loss\n",
    "    \n",
    "    for step in range(num_steps):\n",
    "        optimizer.step(closure)\n",
    "        \n",
    "        if step % 50 == 0:\n",
    "            print(f\"Step {step}\")\n",
    "            imshow(target_img, title='Output Image')\n",
    "\n",
    "    return target_img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8af5c1-5fa3-4c54-81f8-475d019364b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = run_style_transfer(content, style)\n",
    "imshow(output, title='Final Output Image')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a38e1c4-a584-4b17-8bcf-dfc114358f34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DSP",
   "language": "python",
   "name": "dsp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
